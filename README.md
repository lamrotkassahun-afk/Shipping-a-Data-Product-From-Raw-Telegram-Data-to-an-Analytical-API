# Shipping-a-Data-Product-From-Raw-Telegram-Data-to-an-Analytical-API
An end-to-end data pipeline for Telegram, leveraging dbt for transformation, Dagster for orchestration, and YOLOv8 for data enrichment.
Since you are approaching your Tuesday, Jan 20, 2026, 8:00 PM UTC deadline, here is a professionally structured README.md. It covers the hybrid folder structure we fixed, the Dagster orchestration, and the technical stack you used.

Shipping a Data Product: From Raw Telegram Data to an Analytical API
This project implements an end-to-end data engineering pipeline that scrapes medical-related data from Telegram channels, enriches it using Computer Vision (YOLOv11), transforms it into a Star Schema using dbt, and serves the final product via a FastAPI. The entire workflow is orchestrated using Dagster.

ğŸš€ Project Overview
The pipeline follows a Medallion Architecture:

Bronze (Raw): Telegram messages and images collected via Telethon.

Silver (Enriched): Object detection metadata generated by YOLOv11.

Gold (Analytical): Final Star Schema modeled in PostgreSQL via dbt.

ğŸ“‚ Project Structure
Plaintext

â”œâ”€â”€ .venv/                # Virtual environment
â”œâ”€â”€ data/                 # Local storage for raw images and JSON data
â”œâ”€â”€ scripts/              # Ingestion and Orchestration scripts
â”‚   â”œâ”€â”€ telegram.py       # Telegram scraper (Telethon)
â”‚   â”œâ”€â”€ load_to_postgres.py # Data loader to PostgreSQL
â”‚   â””â”€â”€ pipeline.py       # Dagster pipeline definition
â”œâ”€â”€ src/                  # AI and Enrichment logic
â”‚   â””â”€â”€ yolo_detect.py    # YOLOv11 object detection script
â”œâ”€â”€ medical_warehouse/    # dbt project folder
â”‚   â”œâ”€â”€ models/           # SQL transformation models
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ api/                  # FastAPI implementation
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ .env                  # API credentials and DB connection strings
â””â”€â”€ README.md
ğŸ› ï¸ Tech Stack
Orchestration: Dagster

Inference: YOLOv11 (Ultralytics)

Transformation: dbt (Data Build Tool)

Database: PostgreSQL

API: FastAPI

Scraping: Telethon (Telegram API)

âš™ï¸ Installation & Setup
1. Clone the Repository
Bash

git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
2. Set Up Environment
Create a .env file in the root directory:

Code snippet

TG_API_ID=your_id
TG_API_HASH=your_hash
DB_HOST=localhost
DB_NAME=medical_db
DB_USER=postgres
DB_PASS=your_password
3. Install Dependencies
Bash

python -m venv .venv
source .venv/bin/Scripts/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
ğŸš¦ Running the Pipeline
Step 1: Manual Authentication (Required Once)
Telegram requires a one-time OTP for the session. Run the scraper manually once to generate the .session file:

Bash

python scripts/telegram.py
Step 2: Launch Dagster Orchestration
Start the Dagster UI to manage the daily schedule and monitor the pipeline steps:

Bash

dagster dev -f scripts/pipeline.py
Open http://localhost:3000

Navigate to Jobs -> medical_data_pipeline -> Launchpad

Click Launch Run

ğŸ“Š Data Modeling (Star Schema)
The transformation layer converts raw messages into an analytical Star Schema:

Fact Table: fact_messages (links all dimensions and counts detections).

Dimensions: dim_channels, dim_medical_objects, dim_date.

ğŸ›£ï¸ API Endpoints
Once the pipeline finishes, start the API:

Bash

uvicorn api.main:app --reload
View the interactive documentation at http://localhost:8000/docs.

ğŸ“ Challenges & Learnings
Challenge: Managing environmental paths and CWD (Current Working Directory) within Dagster subprocesses.

Learning: The importance of using explicit virtual environment paths in automation to ensure library availability.

Improvement: Implementing real-time event listeners instead of batch processing for faster data availability.